% ============================================================================
% Credit Card Default Prediction - LaTeX Paper Template
% Author: [Your Name]
% Course: [Course Name]
% Date: \today
% ============================================================================

\documentclass[12pt,letterpaper]{article}

% ============================================================================
% PACKAGES
% ============================================================================

% Page layout
\usepackage[margin=1in]{geometry}
\usepackage{setspace}

% Fonts and text
\usepackage{times}  % Times New Roman font
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Math and symbols
\usepackage{amsmath,amssymb}

% Graphics and figures
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}

% Tables
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}

% Colors
\usepackage{xcolor}

% Hyperlinks
\usepackage[hidelinks]{hyperref}

% Bibliography
\usepackage[style=apa,backend=biber]{biblatex}
\addbibresource{references.bib}

% Line spacing (1.5)
\setstretch{1.5}

% Paragraph settings
\setlength{\parindent}{0.5in}
\setlength{\parskip}{0pt}

% ============================================================================
% TITLE PAGE
% ============================================================================

\title{Predicting Credit Card Default with Machine Learning: A Comparative Analysis of Model Performance and Explainability}

\author{
	Hoang To (ht8758) \\
	The University of Texas at Austin \\
	\\
}

\date{\text{December 01, 2025}}

% ============================================================================
% DOCUMENT
% ============================================================================

\begin{document}
	
	\maketitle	

	%\newpage
	
	% ============================================================================
	% ABSTRACT
	% ============================================================================
	
	\begin{abstract}
		% Write your abstract here (200-250 words)
		% Include:
		% - Research problem/motivation
		% - Methods used (5 ML models)
		% - Key findings (best model, performance gap)
		% - Main conclusion (interpretability tradeoff)
		
		Credit underwriting is the life blood of financial institutions. However, due to regulatory requirements for model interpretability, financial institutions currently do not use the state of the art machine learning algorithms in underwriting credit risk, namely neural network based. This study sought to understand how much underwriting performance, and lending profitability, banks are giving up to be in compliance with regulatory bodies. We use data from Coursera's Credit Default challenge, which contains 2xx,xxx data points. I conducted a comprehensive comparative analysis of four machine learning models— logistic regression, XGBoost, LightGBM, and neural networks—evaluating their predictive performance and explainability using SHAP. The results indicate that XGBoost outperforms other models with an ROC-AUC of [X.XXX], while logistic regression, the most interpretable model, achieves an ROC-AUC of [X.XXX], revealing a performance gap of [X.XXX] ([XX.X\%]). Additionally, SHAP analysis demonstrated consistent feature importance across models, suggesting that explainable AI techniques can enhance the interpretability of complex models. However, the tradeoff between performance and interpretability remains a significant consideration for practitioners. The findings provide actionable insights for financial institutions seeking to balance regulatory compliance with predictive accuracy in credit risk assessment.
		
		\noindent
		\textbf{Keywords:} Credit risk, machine learning, explainable AI, SHAP, default prediction, interpretability
	\end{abstract}
	
	\newpage
	
	% ============================================================================
	% 1. INTRODUCTION
	% ============================================================================
	
	\section{Introduction}
	
	% Background and Motivation (2-3 paragraphs)
	Since the dawn of banking, credit has been the core business through which financial institutions grow and compete with one another. The credit card industry operates at the intersection of risk management and customer access to credit. As a finance professional at a leading lender, I experience first hands how our capacity to lend ties directly with our ability to underwrite consumer risk appropriate. Financial institutions must accurately predict which borrowers are likely to default while simultaneously maintaining transparency in their lending decisions. This dual challenge has intensified with the advancement of machine learning techniques that offer superior predictive accuracy but often lack the interpretability required for regulatory compliance. 
	
	% The Regulatory Challenge (1-2 paragraphs)
	The Equal Credit Opportunity Act (ECOA) mandates that lenders provide specific reasons for adverse credit decisions \parencite{ecoa1974}. This regulatory requirement has created a significant barrier to adopting advanced machine learning models, particularly neural networks, which are often considered ``black boxes'' due to their complex internal mechanisms \parencite{hurley2016credit}. As a result, many financial institutions continue to rely on logistic regression and tree-based models despite potentially sacrificing predictive performance [NEED SOURCE HERE].
	
	% Research Questions
	This paper aims to understand two fundamental questions: (1) Are neural network based models outperforming more interpretable logistics and tree-based models? and (2) Can SHAP (SHapley Additive exPlanations) provide sufficient interpretability for complex models to meet regulatory requirements? [CAN WE EVEN USE THIS]. Accurate credit worthiness assessments would help both the lenders and borrowers, whereas the former can assign appropriate level of risk to the borrowers and lower the risk of unexpected losses, allowing the latter to borrow more cheaply \parencite{einav2013impact}.  
	
	% Contribution and Paper Structure
	By comparing four distinct machine learning models, ranging from highly interpretable model (logistic regression) to the ``black box'' model (neural networks), and employing SHAP values for explainability analysis, this paper seeks to provide a repeatable framework to quantify the interpretability-performance tradeoff that financial institutions face, providing actionable insights for practitioners navigating the balance between model performance and regulatory compliance.
	
	% ============================================================================
	% 2. LITERATURE REVIEW
	% ============================================================================
	
	\section{Research Background}
	
	%\subsection{Traditional Credit Scoring Methods}
	
	Credit scoring has been a key pillar of lending decisions for decades. Traditional approaches, particularly FICO scores and logistic regression models, have dominated the industry due to their interpretability and regulatory acceptance \parencite{baesens2003benchmarking}. These methods rely on a limited set of financial and demographic variables to predict default probability, with coefficients that can be directly interpreted as the marginal effect of each variable.
	
	%\subsection{Machine Learning in Credit Risk Assessment}
	
	Within the past two decades, the application of machine learning to credit risk has evolved significantly. Comparative studies demonstrated that ensemble methods could outperform traditional logistic regression \parencite{khandani2010consumer}. Random forests and gradient boosting have emerged as powerful ensemble methods for credit scoring. \textcite{wang2011comparative} found that ensemble learning approaches consistently outperformed single classifiers in credit risk assessment. XGBoost, a scalable tree boosting system \parencite{chen2016xgboost}, has become particularly popular due to its performance and built-in handling of missing values.
	
	Deep learning applications in credit risk have shown promising results but face adoption challenges. \textcite{sirignano2016deep} demonstrated the potential of deep learning for mortgage risk prediction, while \textcite{hamori2018ensemble} compared ensemble learning with deep learning approaches for default risk analysis.
	%\subsection{Explainable AI in Financial Services}
	
	The need for model interpretability in regulated industries has driven significant research in explainable AI (XAI). \textcite{lundberg2017unified} introduced SHAP (SHapley Additive exPlanations), a unified framework for interpreting model predictions based on cooperative game theory. This approach assigns each feature an importance value for a particular prediction, providing both local (individual prediction) and global (overall model) explanations. \textcite{lundberg2020local, molnar2020interpretable} extended their work by demonstrating how SHAP values can provide global understanding of tree-based models. \textcite{bussmann2021explainable} specifically examined explainable machine learning in credit risk management, arguing that SHAP and similar techniques could bridge the gap between model performance and regulatory requirements. In this research, we will use SHAP to gauge the interpretability across machine learning models.

	
	% ============================================================================
	% 3. DATA
	% ==========================================================================
	
	\section{Data}
		
	In aimming to addresses the research questions, this paper utilizes the \href{https://www.coursera.org/projects/data-science-coding-challenge-loan-default-prediction}{Loan Default Prediction dataset from Coursera's Loan Default Prediction Challenge}, which contains borrower information for credit default prediction. The dataset includes 255,347 observations with 16 columns describing a borrower's characteristics such as age, income, credit score, education, etc. that will be used as features (Table \ref{tab:features}). Last but not least, there is a binary target variable indicating default status.
	
	% Add a table of features
	\begin{table}[H]
		\centering
		\caption{Dataset Features}
		\label{tab:features}
		\begin{tabular}{lll}
			\toprule
			\textbf{Feature} & \textbf{Type} & \textbf{Description} \\
			\midrule
			Age & Numerical & Age of the borrower \\
			Income & Numerical & Annual income \\
			LoanAmount & Numerical & Amount of money borrowed \\
			CreditScore & Numerical & Credit score (creditworthiness) \\
			MonthsEmployed & Numerical & Months of employment \\
			NumCreditLines & Numerical & Number of open credit lines \\
			InterestRate & Numerical & Loan interest rate \\
			LoanTerm & Numerical & Loan term in months \\
			DTIRatio & Numerical & Debt-to-Income ratio \\
			Education & Categorical & Highest education level \\
			EmploymentType & Categorical & Employment status \\
			MaritalStatus & Categorical & Marital status \\
			HasMortgage & Categorical & Mortgage status (Yes/No) \\
			HasDependents & Categorical & Has dependents (Yes/No) \\
			LoanPurpose & Categorical & Purpose of the loan \\
			HasCoSigner & Categorical & Co-signer status (Yes/No) \\
			\bottomrule
		\end{tabular}
		
	\end{table}
	\subsection{Exploratory Data Analysis}
	
	Prior to apply any algorithms, we looked through the dataset to have a gasp of the features being leveraged to answer the research questions (Table \ref{tab:summary_stats}). The dataset composed of loans being made to a diverse range of borrowers (income ranges from \$15,000 to \$150,000, loan amount from \$5,000 to \$250,000, etc.). Fortunately, there was no missing-values among the features. 
	
	\begin{table}[htbp]
	\centering
	\caption{Summary Statistics}
	\label{tab:summary_stats}
	\resizebox{\textwidth}{!}{%
	\begin{tabular}{lrrrrrrrrrr}
	\toprule
	& Age & Income & LoanAmount & CreditScore & MonthsEmployed & NumCreditLines & InterestRate & LoanTerm & DTIRatio \\
	\midrule
	mean & 43.50 & 82,499 & 127,579 & 574.26 & 59.54 & 2.50 & 13.49 & 36.03 & 0.50  \\
	std & 14.99 & 38,963 & 70,841 & 158.90 & 34.64 & 1.12 & 6.64 & 16.97 & 0.23  \\
	min & 18.00 & 15,000 & 5,000 & 300.00 & 0.00 & 1.00 & 2.00 & 12.00 & 0.10  \\
	25\% & 31.00 & 48,826 & 66,156 & 437.00 & 30.00 & 2.00 & 7.77 & 24.00 & 0.30  \\
	50\% & 43.00 & 82,466 & 127,556 & 574.00 & 60.00 & 2.00 & 13.46 & 36.00 & 0.50  \\
	75\% & 56.00 & 116,219 & 188,985 & 712.00 & 90.00 & 3.00 & 19.25 & 48.00 & 0.70  \\
	max & 69.00 & 149,999 & 249,999 & 849.00 & 119.00 & 4.00 & 25.00 & 60.00 & 0.90  \\
	\bottomrule
	\end{tabular}%
	}
	\end{table}

	Lastly, the default rate within the dataset is around 11.6\%, which raised a question of imbalanced data, which will be discussed in Section \ref{sec:imbalance}.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{figures/default_distribution.png}
		\caption{Distribution of Default vs Non-Default Cases}
		\label{fig:default_dist}
	\end{figure}
	


	\subsection{Data Preprocessing}\label{sec:preprocessing}
	\subsubsection{Correlated Features}
	To identify multicollinearity among the numerical features, a correlation matrix was computed (Figure \ref{fig:correlation_matrix}). Features with correlation coefficients above 0.8 would be considered highly correlated. In this dataset, we did not see any features with correlation coefficients higher than 0.2, therefore no numerical feature was dropped from the analysis.

	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{figures/correlated_heatmap.png}
		\caption{Feature Correlation Heatmap}
		\label{fig:correlation_matrix}
	\end{figure}
	
	\subsubsection{Normalization and Encoding}
	Subsequently, the following steps were applied to preprocess the data:
	
	\begin{itemize}
		\item \textbf{Train-Test Split:} The dataset was partitioned into training (80\%) and testing (20\%) subsets using stratified sampling to preserve the class distribution of the target variable (Default) in both sets. This is critical given the class imbalance present in the data (approximately 11.6\% default rate). A fixed random state (42) was used to ensure reproducibility across all experiments.

		\item \textbf{Categorical Encoding:} One-hot encoding (dummy variable encoding) was applied to all categorical variables: Education (4 categories: High School, Bachelor's, Master's, PhD), EmploymentType (4 categories: Full-time, Part-time, Self-employed, Unemployed), MaritalStatus (3 categories: Single, Married, Divorced), HasMortgage (2 categories: Yes, No), HasDependents (2 categories: Yes, No), LoanPurpose (5 categories: Home, Auto, Education, Business, Other), and HasCoSigner (2 categories: Yes, No). One-hot encoding was chosen over label encoding to avoid introducing ordinal relationships where none exist, which could mislead tree-based and linear models.

		\item \textbf{Feature Scaling/Normalization:} Numerical features (Age, Income, LoanAmount, CreditScore, MonthsEmployed, NumCreditLines, InterestRate, LoanTerm, DTIRatio) were standardized using z-score normalization:
		\begin{equation}
		z = \frac{x - \mu}{\sigma}
		\end{equation}
		where $\mu$ is the feature mean and $\sigma$ is the standard deviation, computed from the training set only to prevent data leakage. Standardization ensures all features have mean 0 and standard deviation 1, which is essential for gradient-based optimization in logistic regression and neural networks, and improves convergence speed. While tree-based models (XGBoost) are invariant to feature scaling, standardization was applied uniformly for consistency.

		\item \textbf{Final Feature Set:} After preprocessing, the original 16 predictor variables (9 numerical, 7 categorical) were transformed into 24 features: 9 standardized numerical features plus 15 binary indicator variables from one-hot encoding.
	\end{itemize}
	
	\subsection{Class Imbalance Considerations}\label{sec:imbalance}
	
	With a default rate of 11.6\%, there is a noticeable imbalance in the data. Rather than applying resampling techniques like SMOTE \parencite{chawla2002smote}, which can introduce artificial patterns, ROC-AUC was selected as the primary evaluation metric as it is robust to class imbalance and evaluates model performance across all classification thresholds \parencite{fawcett2006introduction}.
	% ============================================================================
	% 4. METHODOLOGY
	% ============================================================================
	
	\section{Methods}
	
	\subsection{Model Selection}
	
	Four machine learning models were selected to represent different levels of interpretability and modeling approaches:
	
	\subsubsection{Tier 1: Highly Interpretable Models}

	\textbf{Logistic Regression:} Selected as the baseline model due to its widespread adoption in the credit industry and regulatory acceptance. Logistic regression models the probability of default using the sigmoid function:
	\begin{equation}
	P(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + ... + \beta_n X_n)}}
	\end{equation}
	where each coefficient $\beta_i$ can be exponentiated to obtain odds ratios, providing direct interpretation: a one-unit increase in feature $X_i$ multiplies the odds of default by $e^{\beta_i}$ \parencite{hosmer2013applied}. This transparency makes logistic regression particularly valuable for regulatory compliance under frameworks such as the Equal Credit Opportunity Act (ECOA) and SR 11-7, which require explainable lending decisions. The model was trained with L2 regularization (C=1.0) and a maximum of 1,000 iterations to ensure convergence.

	\subsubsection{Tier 2: Moderately Interpretable Models}

	\textbf{LightGBM:} A gradient boosting framework developed by Microsoft that uses histogram-based algorithms and leaf-wise tree growth for computational efficiency \parencite{ke2017lightgbm}. Unlike traditional gradient boosting methods that grow trees level-wise, LightGBM grows trees by splitting the leaf with the maximum delta loss, often resulting in deeper, more asymmetric trees that can capture complex interactions. 

	\noindent\textbf{XGBoost (Extreme Gradient Boosting):} A regularized gradient boosting implementation that has become the de facto standard for tabular data competitions and industry applications. XGBoost minimizes a regularized objective function:
	\begin{equation}
	\mathcal{L}(\phi) = \sum_{i} l(\hat{y}_i, y_i) + \sum_{k} \Omega(f_k)
	\end{equation}
	where $l$ is a differentiable convex loss function and $\Omega(f_k) = \gamma T + \frac{1}{2}\lambda ||w||^2$ penalizes model complexity through the number of leaves $T$ and L2 regularization on leaf weights $w$. This built-in regularization helps prevent overfitting, a common concern with high-dimensional credit data. 
	\subsubsection{Tier 3: Low Interpretability Models}

	\textbf{Neural Network:} A feedforward multilayer perceptron (MLP) represents the ``black box'' end of the interpretability spectrum. The architecture consists of an input layer matching the 24 preprocessed features, followed by three hidden layers with 128, 64, and 32 neurons respectively, each using ReLU activation functions:
	\begin{equation}
	\text{ReLU}(x) = \max(0, x)
	\end{equation}
	Dropout regularization (rate = 0.3) was applied between hidden layers to prevent overfitting, and the output layer uses sigmoid activation for binary classification. The network was trained using the Adam optimizer with binary cross-entropy loss and early stopping (patience = 10 epochs) monitoring validation loss. 
	\subsection{Evaluation Metrics}
	
	Model performance was assessed using multiple classification metrics:
	
	\begin{itemize}
		\item \textbf{Accuracy:} Overall proportion of correct predictions
		\item \textbf{Precision:} Ability to minimize false positives (incorrectly predicting default)
		\item \textbf{Recall:} Ability to identify actual defaults
		\item \textbf{F1-Score:} Harmonic mean of precision and recall
		\item \textbf{ROC-AUC:} Area under the receiver operating characteristic curve, used as the primary metric for model comparison due to its robustness to class imbalance
	\end{itemize}
	
	ROC-AUC was selected as the primary metric because it evaluates model performance across all classification thresholds and is less sensitive to class imbalance than accuracy \parencite{he2009learning}.
	

	\subsection{SHAP Implementation}
	
	SHAP values were calculated for all models to enable fair comparison of explainability:
	
	\begin{itemize}
		\item \textbf{LinearExplainer:} For logistic regression, computing exact SHAP values from model coefficients
		\item \textbf{TreeExplainer:} For decision tree, random forest, and XGBoost, leveraging tree structure for efficient computation
		\item \textbf{KernelExplainer:} For neural networks, using model-agnostic approximation on sampled background data
	\end{itemize}
	
	SHAP analysis provides both global feature importance (which features matter most overall) and local explanations (why a specific prediction was made), enabling assessment of whether complex models can achieve sufficient explainability \parencite{lundberg2017unified}.
	
	% ============================================================================
	% 5. RESULTS
	% ============================================================================
	
	\section{Results}
	
	\subsection{Overall Model Performance}
	
	Table \ref{tab:model_performance} presents the performance metrics for all five models on the test set. [Describe the best performing model and overall patterns]
	
	\begin{table}[H]
		\centering
		\caption{Model Performance Comparison}
		\label{tab:model_performance}
		\begin{tabular}{lccccc}
			\toprule
			\textbf{Model} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{ROC-AUC} \\
			\midrule
			Logistic Regression & [X.XXX] & [X.XXX] & [X.XXX] & [X.XXX] & [X.XXX] \\
			Decision Tree & [X.XXX] & [X.XXX] & [X.XXX] & [X.XXX] & [X.XXX] \\
			Random Forest & [X.XXX] & [X.XXX] & [X.XXX] & [X.XXX] & [X.XXX] \\
			XGBoost & [X.XXX] & [X.XXX] & [X.XXX] & [X.XXX] & [X.XXX] \\
			Neural Network & [X.XXX] & [X.XXX] & [X.XXX] & [X.XXX] & [X.XXX] \\
			\bottomrule
		\end{tabular}
	\end{table}
	
	\noindent Key findings from the performance comparison:

	\begin{itemize}
		\item Best performing model with ROC-AUC of X.XXX
		\item Baseline logistic regression achieved ROC-AUC of X.XXX
		\item Ranking and performance differences between models
		\item Notable patterns in precision vs recall tradeoffs
	\end{itemize}
	
	% Add ROC curves figure
	\begin{figure}[H]
		\centering
		\caption{ROC Curves for All Models}
		\label{fig:roc_curves}
	\end{figure}
	
	Figure \ref{fig:roc_curves} displays the ROC curves for all five models, providing visual confirmation of [describe what the figure shows]. The separation between curves indicates [interpretation].
	
	\subsection{Performance vs Interpretability Tradeoff}
	
	One of the central questions of this research concerns the performance cost of interpretability. Table \ref{tab:tradeoff} quantifies this tradeoff:
	
	\begin{table}[H]
		\centering
		\caption{Interpretability-Performance Analysis}
		\label{tab:tradeoff}
		\begin{tabular}{lcc}
			\toprule
			\textbf{Metric} & \textbf{Value} & \textbf{Model} \\
			\midrule
			Best Overall Performance (ROC-AUC) & [X.XXX] & [Model Name] \\
			Best Interpretable Model (ROC-AUC) & [X.XXX] & [Model Name] \\
			\textbf{Performance Gap} & \textbf{[X.XXX] ([XX.X\%])} & --- \\
			\bottomrule
		\end{tabular}
	\end{table}
	
	The performance gap of [X.XXX] (or [XX.X\%]) represents the quantifiable cost of choosing an interpretable model over the highest-performing complex model. This finding has significant implications for practitioners who must balance regulatory compliance with predictive accuracy.
	
	% Add interpretability vs performance scatter plot
	\begin{figure}[H]
		\centering
%		\includegraphics[width=0.75\textwidth]{interpretability_performance_tradeoff.png}
		\caption{Model Performance vs Interpretability Tradeoff}
		\label{fig:tradeoff}
	\end{figure}
	
	Figure \ref{fig:tradeoff} visualizes this tradeoff, with interpretability scores assigned based on model characteristics (5 = highest interpretability for logistic regression and decision trees, 1 = lowest for neural networks). The plot clearly demonstrates [describe the trend shown in the figure].
	
	\subsection{SHAP Analysis Results}
	
	\subsubsection{Global Feature Importance}
	
	SHAP analysis revealed the most influential features for default prediction across all models. Despite differences in model architecture, there was [describe level of agreement/disagreement] across models regarding feature importance.
	
	% Add SHAP summary plots
	\begin{figure}[H]
		\centering
		\begin{subfigure}[b]{0.45\textwidth}
%			\includegraphics[width=\textwidth]{shap_logistic_regression.png}
			\caption{Logistic Regression}
		\end{subfigure}
		\hfill
		\begin{subfigure}[b]{0.45\textwidth}
%			\includegraphics[width=\textwidth]{shap_xgboost.png}
			\caption{XGBoost}
		\end{subfigure}
		\caption{SHAP Summary Plots for Selected Models}
		\label{fig:shap_summary}
	\end{figure}
	
	The top five most important features across models were:
	\begin{enumerate}
		\item \textbf{[Feature Name]:} [Interpretation of its effect]
		\item \textbf{[Feature Name]:} [Interpretation of its effect]
		\item \textbf{[Feature Name]:} [Interpretation of its effect]
		\item \textbf{[Feature Name]:} [Interpretation of its effect]
		\item \textbf{[Feature Name]:} [Interpretation of its effect]
	\end{enumerate}
	
	[Discuss consistency or inconsistency across models, and what this means for explainability]
	
	\subsubsection{Consistency of Explanations}
	
	A critical question for regulatory compliance is whether different models provide consistent explanations for predictions. Comparing SHAP values across models revealed [describe findings about consistency]. This [supports/challenges] the hypothesis that SHAP can provide sufficient explainability for complex models.
	
	\subsection{Model-Specific Insights}
	
	\textbf{Logistic Regression:} As the baseline model, logistic regression achieved [performance]. The most significant predictors were [features], with [describe coefficient magnitudes and directions].
	
	\textbf{Decision Tree:} [Describe performance, key decision rules, tree depth]
	
	\textbf{Random Forest:} [Describe performance, ensemble benefits, feature importance patterns]
	
	\textbf{XGBoost:} [Describe performance, why it performed well/poorly, hyperparameter sensitivity]
	
	\textbf{Neural Network:} [Describe performance relative to expectations, can SHAP make it explainable?]
	
	
	% ============================================================================
	% 6. CONCLUSION
	% ============================================================================
	
	\section{Conclusion}
	
	This study set out to address three fundamental questions about the application of machine learning to credit default prediction: which models perform best, what is the cost of interpretability, and whether explainable AI techniques can bridge the performance-interpretability gap.
	
	\subsection{Summary of Key Findings}
	
	The empirical analysis revealed several important findings:
	
	\begin{enumerate}
		\item \textbf{Best Model:} [Model name] achieved the highest predictive performance with ROC-AUC of [X.XXX], demonstrating that [interpretation of why this model excelled].
		
		\item \textbf{Interpretability Cost:} The performance gap between the best overall model and the best interpretable model was [X.XXX] ([XX.X\%]), quantifying the tangible cost of regulatory compliance.
		
		\item \textbf{SHAP Effectiveness:} SHAP analysis successfully provided interpretable explanations across all models, with [degree of consistency] in feature importance rankings. However, the sufficiency of SHAP for full regulatory compliance remains context-dependent.
	\end{enumerate}
	
	\subsection{Contributions}
	
	This research makes several contributions to both academic literature and industry practice:
	
	\begin{itemize}
		\item \textbf{Quantification of Tradeoff:} Provides empirical evidence of the specific performance sacrifice required for interpretability in credit default prediction.
		
		\item \textbf{SHAP Evaluation:} Systematically evaluates SHAP across multiple model types, assessing its potential to enable complex model deployment.
		
		\item \textbf{Practical Framework:} Offers a replicable methodology for financial institutions to evaluate model choices based on their specific priorities.
	\end{itemize}
	
	\subsection{Practical Recommendations}
	
	For financial institutions navigating the performance-interpretability tradeoff:
	
	\begin{itemize}
		\item \textbf{Conservative approach:} Deploy highly interpretable models ([logistic regression or decision trees]) where regulatory requirements are strict or uncertain. Accept the [X\%] performance cost as insurance against compliance risks.
		
		\item \textbf{Innovative approach:} Utilize high-performance models ([model name]) with comprehensive SHAP analysis for explanations. Engage proactively with regulators to establish the acceptability of SHAP-based adverse action notices.
		
		\item \textbf{Balanced approach:} Adopt ensemble methods ([Random Forest or XGBoost]) that offer strong performance ([within X\% of best model]) while maintaining moderate interpretability through feature importance and SHAP.
	\end{itemize}
	
	\subsection{Final Thoughts}
	
	The future of credit scoring lies not in choosing between performance and interpretability, but in developing methods that optimize both simultaneously. Explainable AI techniques like SHAP represent a promising step toward this goal, offering mathematical rigor and practical utility. However, the ultimate acceptability of these methods depends on regulatory evolution and demonstrated consumer understanding.
	
	As machine learning continues to advance, the credit industry must balance three competing imperatives: maximizing predictive accuracy to manage risk, maintaining transparency to satisfy regulations, and ensuring fairness to serve all consumers equitably. This study demonstrates that while tradeoffs exist, they need not be as stark as commonly assumed. With appropriate explanation methods, the gap between high-performing and interpretable models can be narrowed, enabling financial institutions to serve their customers and stakeholders more effectively.
	
	The [X\%] performance gap quantified in this research represents not a fixed constraint but rather the current state of technology and regulation. As explainability methods improve and regulatory frameworks evolve to accommodate them, this gap may narrow further. The path forward requires continued collaboration between researchers, practitioners, and regulators to develop credit scoring systems that are simultaneously accurate, interpretable, and fair.
	
	% ============================================================================
	% REFERENCES
	% ============================================================================
	
	\newpage
	%\nocite{*}
	\printbibliography[title=References]
	

	
\end{document}
